import os
import streamlit as st
from langchain.chat_models import ChatOpenAI
import oss2
import logging
from config import *
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document

import config

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# åˆ›å»ºBucketå¯¹è±¡ï¼Œæ‰€æœ‰Objectç›¸å…³çš„æ¥å£éƒ½å¯ä»¥é€šè¿‡Bucketå¯¹è±¡æ¥è¿›è¡Œ
from config import EMBEDDING_MODEL

if UPLOAD_OSS:
    access_key_id = config.OSS_ACCESS_KEY_ID
    access_key_secret = config.OSS_ACCESS_KEY_SECRET
    auth = oss2.Auth(access_key_id, access_key_secret)
    bucket = oss2.Bucket(auth, config.ENDPOINT, config.BUCKET_NAME)

MAX_FILE_SIZE_BYTES = 2 * 1024 * 1024  # 2MB

upload_file_status = st.session_state.get("uploaded_file", "")

vector = ""

prompt = ChatPromptTemplate.from_template("""
                      è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚
                      æˆ‘å°†ä¼šä¸ºä½ æä¾›ä¸€äº›å…³äºé—®é¢˜çš„å·²çŸ¥ä¿¡æ¯ï¼Œè¯·æ ¹æ®æˆ‘æ‰€æä¾›çš„å·²çŸ¥ä¿¡æ¯ä½œä¸ºå›ç­”çš„ä¾æ®ï¼Œæ³¨æ„ä¸èƒ½æé€ è™šå‡ç­”æ¡ˆï¼Œå¹¶æ³¨æ„å›ç­”è¯­å¥çš„æµç•…ä»¥åŠé€»è¾‘é€šé¡ºï¼Œä¸èƒ½å‰åçŸ›ç›¾ã€‚å¦‚æœæ— æ³•å›ç­”ï¼Œè¯·è¯´ "æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•æ‰¾åˆ°ç­”æ¡ˆ"ã€‚
                      è‹¥æˆ‘ç³»ç»Ÿçš„å·²çŸ¥ä¿¡æ¯ä¸ºç©ºæˆ–è€…æ— æ•ˆï¼Œè¯·åœ¨å›ç­”çš„å¥å­å‰åŠ ä¸Šè¿™æ®µæ¶ˆæ¯:"çŸ¥è¯†åº“æ— æ³•åŒ¹é…åˆ°æœ‰æ•ˆä¿¡æ¯ä»¥ä¸‹ä¸ºå¤§æ¨¡å‹è‡ªèº«èƒ½åŠ›å›ç­”\n\n
                       <å·²çŸ¥ä¿¡æ¯>
                        {context}
                        </å·²çŸ¥ä¿¡æ¯>
                        Question: {input}
                  """)


def on_submit():
    user_input = st.session_state['input_chat']
    vector = st.session_state.get('vector', "")
    openai_api_key = st.session_state.get('openai_api_key')
    do_query(user_input, vector, openai_api_key)


def on_change():
    uploaded_file = st.session_state['uploaded_file']
    openai_api_key = st.session_state['openai_api_key']
    embedding_model = st.session_state['embedding_model']
    uploaded_files(uploaded_file,openai_api_key,embedding_model)


with st.sidebar:
    openai_api_key = st.text_input("Openai API Key", key="openai_api_key", type="password")
    openai_model = st.selectbox("è¯·é€‰æ‹©æ‰€éœ€llmæ¨¡å‹", ["gpt-4"], 0, key="openai_model")
    embedding_model = st.selectbox("è¯·é€‰æ‹©æ‰€éœ€embeddingæ¨¡å‹", EMBEDDING_MODEL, 0, key="embedding_model")
    "[![Open with GitHub](https://github.com/codespaces/badge.svg)](https://github.com/xucailiang/rabbitchat)"


st.title("ğŸ“ File Q&A with Anthropic")
uploaded_file = st.file_uploader("Upload an article", type=("pdf"), on_change=on_change, key="uploaded_file")
question = st.chat_input(
    "è¯·è¾“å…¥å¯¹è¯å†…å®¹ï¼Œæ¢è¡Œè¯·ä½¿ç”¨Shift+Enter",
    on_submit=on_submit,
    key="input_chat"
)
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if "vector" not in st.session_state:
    st.error("è¯·å…ˆä¸Šä¼ æ–‡ä»¶æ‰èƒ½æé—®å™¢")


if uploaded_file and question and not openai_api_key:
    st.info("Please add your Anthropic API key to continue.")


def do_query(question, vector, openai_api_key):
    if not openai_api_key:
        st.error("Please add your Openai API key to continue.")
    elif vector:
        st.session_state.messages.append({"role": "user", "content": question})
        retriever = vector.as_retriever()
        llm = ChatOpenAI(model=openai_model, openai_api_key=openai_api_key)
        document_chain = create_stuff_documents_chain(llm, prompt)
        retrieval_chain = create_retrieval_chain(retriever, document_chain)
        response = retrieval_chain.invoke({"input": question})
        res_context = ""
        for document in response['context']:
            metadata = document.metadata
            source = metadata['source']
            page = metadata['page']
            res_context += f"> {source} ç¬¬{page}é¡µ \n\n"
        st.session_state.messages.append({"role": "assistant", "content": f"{response['answer']}"
                                                                          f"\n\nç­”æ¡ˆå‡ºè‡ª \n\n"
                                                                          f"{res_context}"})


def uploaded_files(uploaded_file, openai_api_key, embedding_model):
    uploaded_file_name = uploaded_file.name
    current_directory = os.getcwd()
    file_tmp_path = os.path.join(current_directory, uploaded_file_name)
    if openai_api_key:
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦è¶…è¿‡é™åˆ¶
            if uploaded_file.size > MAX_FILE_SIZE_BYTES:
                st.error(f"ä¸Šä¼ çš„æ–‡ä»¶è¶…è¿‡æœ€å¤§é™åˆ¶ï¼ˆ{MAX_FILE_SIZE_BYTES / (1024 * 1024)} MBï¼‰", timeout=3)
            else:
                with open(file_tmp_path, "wb") as f:
                    f.write(uploaded_file.read())
                if UPLOAD_OSS:
                    result = bucket.put_object_from_file(uploaded_file_name, file_tmp_path, headers={"Content-Disposition": "attachment"})
                    logger.info(f"æ–‡ä»¶{uploaded_file_name}ä¸Šä¼ æˆåŠŸ!Etag:{result.etag},request_id:{result.request_id}")
                embeddings = OpenAIEmbeddings(model=embedding_model, openai_api_key=openai_api_key)
                docs = PyMuPDFLoader(file_tmp_path).load()
                text_splitter = RecursiveCharacterTextSplitter()
                documents = text_splitter.split_documents(docs)
                vector = FAISS.from_documents(documents, embeddings)
                st.session_state['vector'] = vector
        finally:
            os.remove(file_tmp_path)
            logger.info(f"æ–‡ä»¶åˆ é™¤å®Œæˆ")
    else:
        st.error("è¯·ç¡®è®¤openaiæ˜¯å¦è¾“å…¥æ­£ç¡®")
