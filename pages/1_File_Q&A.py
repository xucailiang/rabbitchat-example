import os
import streamlit as st
from langchain.chat_models import ChatOpenAI
import logging
from config import *
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

os.environ["OPENAI_API_BASE"] = 'https://gateway.ai.cloudflare.com/v1/92a11adae8e8640ee190fde50328431e/open-ai/openai'

import config

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# åˆ›å»ºBucketå¯¹è±¡ï¼Œæ‰€æœ‰Objectç›¸å…³çš„æ¥å£éƒ½å¯ä»¥é€šè¿‡Bucketå¯¹è±¡æ¥è¿›è¡Œ
from config import EMBEDDING_MODEL

if UPLOAD_OSS:
    import oss2
    access_key_id = config.OSS_ACCESS_KEY_ID
    access_key_secret = config.OSS_ACCESS_KEY_SECRET
    auth = oss2.Auth(access_key_id, access_key_secret)
    bucket = oss2.Bucket(auth, config.ENDPOINT, config.BUCKET_NAME)

MAX_FILE_SIZE_BYTES = 5 * 1024 * 1024  # 5MB


prompt = ChatPromptTemplate.from_template("""
                      è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚
                      æˆ‘å°†ä¼šä¸ºä½ æä¾›ä¸€äº›å…³äºé—®é¢˜çš„å·²çŸ¥ä¿¡æ¯ï¼Œè¯·å®Œå…¨æ ¹æ®æˆ‘æ‰€æä¾›çš„å·²çŸ¥ä¿¡æ¯ä½œä¸ºå›ç­”çš„ä¾æ®ï¼Œæ³¨æ„ä¸èƒ½æé€ è™šå‡ç­”æ¡ˆï¼Œå¹¶æ³¨æ„å›ç­”è¯­å¥çš„æµç•…ä»¥åŠé€»è¾‘é€šé¡ºï¼Œä¸èƒ½å‰åçŸ›ç›¾ã€‚å¦‚æœæ— æ³•å›ç­”ï¼Œè¯·è¯´ "æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•æ‰¾åˆ°ç­”æ¡ˆ"ã€‚
                      è‹¥æˆ‘æä¾›çš„å·²çŸ¥ä¿¡æ¯ä¸ºç©ºæˆ–è€…æ— æ•ˆï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„çŸ¥è¯†å›ç­”ï¼Œä½†æ˜¯å¿…é¡»åœ¨å›ç­”çš„å¥å­å‰åŠ ä¸Šè¿™æ®µè¯:"çŸ¥è¯†åº“æ— æ³•åŒ¹é…åˆ°æœ‰æ•ˆä¿¡æ¯ï¼Œä»¥ä¸‹ä¸ºå¤§æ¨¡å‹è‡ªèº«èƒ½åŠ›å›ç­”\n\n"ã€‚
                       <å·²çŸ¥ä¿¡æ¯>
                        {context}
                        </å·²çŸ¥ä¿¡æ¯>
                        Question: {input}
                  """)


def on_submit():
    vector_store = st.session_state.get('vector_store', "")
    question = st.session_state.get('input_chat', "")
    do_query(vector_store, question)


def on_change():
    uploaded_file = st.session_state['uploaded_file']
    uploaded_files(uploaded_file)


def get_embeddings_model():
    logger.info(f"embeddingæ¨¡å‹: {embedding_model}")
    print(f"embeddingæ¨¡å‹: {embedding_model}")
    if embedding_model == "BAAI/bge-small-zh-v1.5":
        from langchain.embeddings import HuggingFaceBgeEmbeddings
        embeddings = HuggingFaceBgeEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': 'auto'},
            encode_kwargs={'normalize_embeddings': True},
            query_instruction="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
        )
    else:
        embeddings = OpenAIEmbeddings(model=embedding_model, openai_api_key=openai_api_key)
    return embeddings


def upload_to_aliyunoss(uploaded_file_name, file_tmp_path):
    result = bucket.put_object_from_file(uploaded_file_name, file_tmp_path,
                                         headers={"Content-Disposition": "attachment"})
    logger.info(f"æ–‡ä»¶{uploaded_file_name}ä¸Šä¼ æˆåŠŸ!Etag:{result.etag},request_id:{result.request_id}")


with st.sidebar:
    openai_api_key = st.text_input("Openai API Key", key="openai_api_key", type="password")
    openai_model = st.selectbox("è¯·é€‰æ‹©æ‰€éœ€llmæ¨¡å‹", ["gpt-4"], 0, key="openai_model")
    embedding_model = st.selectbox("è¯·é€‰æ‹©æ‰€éœ€embeddingæ¨¡å‹", EMBEDDING_MODEL, 0, key="embedding_model")
    "[![Open with GitHub](https://github.com/codespaces/badge.svg)](https://github.com/xucailiang/rabbitchat-example)"


st.title("ğŸ“ File Q&A with Anthropic")

uploaded_file = st.file_uploader("Upload an article", type=("pdf"), on_change=on_change, key="uploaded_file", disabled=not openai_api_key)

question = st.chat_input(
    "è¯·è¾“å…¥å¯¹è¯å†…å®¹ï¼Œæ¢è¡Œè¯·ä½¿ç”¨Shift+Enter",
    on_submit=on_submit,
    key="input_chat"
)
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if "vector_store" not in st.session_state:
    st.error("è¯·å…ˆä¸Šä¼ æ–‡ä»¶æ‰èƒ½æé—®å™¢")


if uploaded_file and question and not openai_api_key:
    st.info("Please add your Anthropic API key to continue.")


def do_query(vector_store, question):
    if not openai_api_key:
        st.error("Please add your Openai API key to continue.")
    elif vector_store:
        st.session_state.messages.append({"role": "user", "content": question})
        retriever = vector_store.as_retriever()
        llm = ChatOpenAI(model=openai_model, openai_api_key=openai_api_key)
        document_chain = create_stuff_documents_chain(llm, prompt)
        retrieval_chain = create_retrieval_chain(retriever, document_chain)
        print(f"question: {question}")
        response = retrieval_chain.invoke({"input": question})
        res_context = ""
        for document in response['context']:
            metadata = document.metadata
            source = metadata['source']
            page = metadata['page']
            res_context += f"> {source} ç¬¬{page}é¡µ \n\n"
        st.session_state.messages.append({"role": "assistant", "content": f"{response['answer']}"
                                                                          f"\n\nç­”æ¡ˆå‡ºè‡ª \n\n"
                                                                          f"{res_context}"})


def uploaded_files(uploaded_file):
    file_tmp_path = ""
    if openai_api_key and uploaded_file:
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦è¶…è¿‡é™åˆ¶
            if uploaded_file.size > MAX_FILE_SIZE_BYTES:
                st.error(f"ä¸Šä¼ çš„æ–‡ä»¶è¶…è¿‡æœ€å¤§é™åˆ¶ï¼ˆ{MAX_FILE_SIZE_BYTES / (1024 * 1024)} MBï¼‰")
                return
            else:
                uploaded_file_name = uploaded_file.name
                current_directory = os.getcwd()
                file_tmp_path = os.path.join(current_directory, uploaded_file_name)

                with open(file_tmp_path, "wb") as f:
                    f.write(uploaded_file.read())
                if UPLOAD_OSS:
                    upload_to_aliyunoss(uploaded_file_name, file_tmp_path)

                embeddings = get_embeddings_model()
                docs = PyMuPDFLoader(file_tmp_path).load()
                text_splitter = RecursiveCharacterTextSplitter()
                documents = text_splitter.split_documents(docs)
                vector_store = FAISS.from_documents(documents, embeddings)
                st.session_state['vector_store'] = vector_store
                st.session_state.pop("uploaded_file") # å¦‚æœä¸æ‰‹åŠ¨æ¸…ç†ï¼Œç¬¬äºŒæ¬¡ä¸Šä¼ æ–‡ä»¶å°±ä¼šæŠ¥é”™

                # docs = PyMuPDFLoader(file_tmp_path).load()
                # parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)
                # child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)
                # # todo: å¤šç”¨æˆ·å¯èƒ½ä¼šäº§ç”Ÿå¤šæ•°æ®åº“çš„é—®é¢˜ï¼Œéœ€è¦åŠæ—¶é”€æ¯ã€‚
                # vector_store = Chroma(collection_name="knowledge", embedding_function=embeddings)
                # # åˆ›å»ºå†…å­˜å­˜å‚¨å¯¹è±¡
                # store = InMemoryStore()
                # retriever = ParentDocumentRetriever(
                #     vectorstore=vector_store,
                #     docstore=store,
                #     child_splitter=child_splitter,
                #     parent_splitter=parent_splitter,
                # )
                # retriever.add_documents(docs)

        finally:
            if file_tmp_path:
                os.remove(file_tmp_path)
                logger.info(f"æ–‡ä»¶åˆ é™¤å®Œæˆ")
    else:
        st.error("è¯·ç¡®è®¤openaiæ˜¯å¦è¾“å…¥æ­£ç¡®")
